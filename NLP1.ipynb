{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189DTgmrnnBW"
      },
      "source": [
        "# Testing spaCy Framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9tnQzB-SbzR",
        "outputId": "00b5ee88-656d-499e-ef92-704158347988"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 284kB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.0MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.4MB/s \n",
            "\u001b[?25hCollecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 57.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=12b8affed7957cf934a6299a6c6cb45a0bac0ad9fe2bd46510c336422dced2ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, srsly, pydantic, thinc, smart-open, typer, pathy, spacy-legacy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqL6okhLSiwv",
        "outputId": "d1c046e9-04b9-4595-cfe9-e7d9dc896eb3"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 09:37:09.182668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 326kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX_ydyAfSCT3",
        "outputId": "c0c28c1b-b20c-40ae-9fbf-b579a6036bb6"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n",
            "Sebastian NORP\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwZ9_yUj2ppd"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_entity(text):\n",
        "  # output all the entity that text contains\n",
        "  doc = nlp(text)\n",
        "  for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ZSawxC2XCM",
        "outputId": "54e1f843-db77-4837-88bc-c5285023da58"
      },
      "source": [
        "texts = [\"Apache Kafka 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.2.0, 2.2.1, or 2.3.0 are configured with one or more config providers\",\n",
        "        \"Users should upgrade to 2.2.2 or higher, or 2.3.1 or higher where this vulnerability has been fixed.\",\n",
        "         \"In Apache Kafka versions between 0.11.0.0 and 2.1.0, it is possible to manually craft a Produce request which bypasses transaction/idempotent ACL validation.\",\n",
        "         \"Potential HTTP request smuggling in Apache Tomcat\",\n",
        "         \"OpenStack Identity (Keystone) Folsom 2012.2.4 and earlier, Grizzly before 2013.1.1, and Havana does not immediately revoke the authentication token when deleting a user through the Keystone v2 API, which allows remote authenticated users to retain access via the token.\"]\n",
        "\n",
        "print(\"==========\")\n",
        "for text in texts:\n",
        "  get_entity(text)\n",
        "  print(\"==========\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========\n",
            "Apache Kafka PERSON\n",
            "2.0.1 DATE\n",
            "2.1.1 DATE\n",
            "2.2.0 DATE\n",
            "2.2.1 DATE\n",
            "2.3.0 DATE\n",
            "one CARDINAL\n",
            "==========\n",
            "2.2.2 CARDINAL\n",
            "2.3.1 CARDINAL\n",
            "==========\n",
            "Apache Kafka PRODUCT\n",
            "2.1.0 CARDINAL\n",
            "ACL ORG\n",
            "==========\n",
            "==========\n",
            "OpenStack Identity (Keystone ORG\n",
            "2012.2.4 CARDINAL\n",
            "Havana GPE\n",
            "the Keystone v2 API FAC\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmWCh0TrnuuA"
      },
      "source": [
        "# Importing Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDZ0SFdzma1k"
      },
      "source": [
        "IMPORTERS = ['debian']\n",
        "DATA_DIR = '/content/drive/MyDrive/GsoC 2021/Data/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmcvRQ-MSvGv"
      },
      "source": [
        "sentences = []\n",
        "for importer in IMPORTERS:\n",
        "  f = open(DATA_DIR + importer + '.txt', 'r')\n",
        "  sentences.extend(f.read().split('@')) # needs to be small\n",
        "print(len(sentences))\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8GsY-xcPDj"
      },
      "source": [
        "# Sample Code of Transfer Learning on spaCy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhMsuZCVcbAk"
      },
      "source": [
        "'''\n",
        "TP = Word predicted as either Package_Name or Package_Version and present in the data(train/test/validation) as either Package_Name or Package_Version \n",
        "FP = Word predicted as either Package_Name or Package_Version and not present in the data (train/test/validation) as either Package_Name or Package_Version \n",
        "FN = Word present in the data(train/test/validation) data as either Package_Name or Package_Version but not predicted as either Package_Name or Package_Version\n",
        "'''\n",
        "# Precision = TP/(TP+FP )\n",
        "def calc_precision(pred, true):        \n",
        "    precision = len([x for x in pred if x in true]) / (len(pred) + 1e-20)\n",
        "    return precision\n",
        "\n",
        "# Recall = TP/(TP+FN)\n",
        "def calc_recall(pred, true):\n",
        "    recall = len([x for x in true if x in pred]) / (len(true) + 1e-20)\n",
        "    return recall\n",
        "\n",
        "# F1-score = 2 * Precision* Recall/ (Precision+Recall)\n",
        "def calc_f1(precision, recall):\n",
        "    f1 = 2 * ((precision * recall) / (precision + recall + 1e-20))\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUfqWFxvdZMD"
      },
      "source": [
        "# run the predictions on each sentence in the evaluation  dataset, and return the metrics\n",
        "def evaluate(ner, data):\n",
        "    preds = [ner(x[0]) for x in data]\n",
        "\n",
        "    precisions, recalls, f1s = [], [], []\n",
        "\n",
        "    # iterate over predictions and test data and calculate precision, recall, and F1-score\n",
        "    for pred, true in zip(preds, data):\n",
        "        true = [x[2] for x in list(chain.from_iterable(true[1].values()))]\n",
        "        pred = [i.label_ for i in pred.ents]\n",
        "        precision = calc_precision(true, pred)\n",
        "        precisions.append(precision)\n",
        "        recall = calc_recall(true, pred)\n",
        "        recalls.append(recall)\n",
        "        f1s.append(calc_f1(precision, recall))\n",
        "\n",
        "    return {\"textcat_p\": np.mean(precisions), \"textcat_r\": np.mean(recalls), \"textcat_f\":np.mean(f1s)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jb6SOVdsUy"
      },
      "source": [
        "def train_spacy(train_data, labels, iterations, dropout = 0.5, display_freq = 1):\n",
        "    ''' Train a spacy NER model, which can be queried against with test data\n",
        "   \n",
        "    train_data : training data in the format of (sentence, {entities: [(start, end, label)]})\n",
        "    labels : a list of unique annotations\n",
        "    iterations : number of training iterations\n",
        "    dropout : dropout proportion for training\n",
        "    display_freq : number of epochs between logging losses to console\n",
        "    '''\n",
        "\n",
        "    valid_f1scores=[]\n",
        "    test_f1scores=[]\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner)\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "   \n",
        "    # Add entity labels to the NER pipeline\n",
        "    for i in labels:\n",
        "        ner.add_label(i)\n",
        "\n",
        "    # Disable other pipelines in SpaCy to only train NER\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itr in range(iterations):\n",
        "            random.shuffle(train_data) # shuffle the training data before each iteration\n",
        "            losses = {}\n",
        "            batches = minibatch(train_data, size = compounding(16.0, 64.0, 1.5))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(          \n",
        "                    texts,\n",
        "                    annotations,\n",
        "                    drop = dropout,  \n",
        "                    sgd = optimizer,\n",
        "                    losses = losses)\n",
        "            \n",
        "            print('=======================================')\n",
        "            print('Interation = '+str(itr))\n",
        "            print('Losses = '+str(losses))\n",
        "\n",
        "            # Evaluate present model on Validation dataset\n",
        "            scores = evaluate(nlp,VALID_DATA)\n",
        "            valid_f1scores.append(scores[\"textcat_f\"])\n",
        "            print('===============VALID DATA========================')\n",
        "            print('F1-score = '+str(scores[\"textcat_f\"]))\n",
        "            print('Precision = '+str(scores[\"textcat_p\"]))\n",
        "            print('Recall = '+str(scores[\"textcat_r\"]))\n",
        "\n",
        "            # evaluate present model on Test dataset\n",
        "            scores = evaluate(nlp,TEST_DATA)\n",
        "            test_f1scores.append(scores[\"textcat_f\"])\n",
        "            print('===============TEST DATA========================')\n",
        "            print('F1-score = '+str(scores[\"textcat_f\"]))\n",
        "            print('Precision = '+str(scores[\"textcat_p\"]))\n",
        "            print('Recall = '+str(scores[\"textcat_r\"]))\n",
        "            print('=======================================')\n",
        "            \n",
        "    return nlp,valid_f1scores,test_f1scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVkB6Dp9n1l7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcZ-ZSZ4n4DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}